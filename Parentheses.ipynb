{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applying formal verification to ensure a language model always outputs balanced parentheses\n",
        "\n",
        "We will train a scaled-down version of GPT-2 to complete sequences of parentheses so that the result is balanced. Then we will use the auto_LiRPA verification library to check that for any input sequence can be balanced, the output will in fact be balanced.\n"
      ],
      "metadata": {
        "id": "6pJPtS1aAzWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/AlejoAcelas/auto_LiRPA.git\n",
        "import numpy as np\n",
        "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
        "# Importing Verification Library"
      ],
      "metadata": {
        "id": "oWT03i-L_ejQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Training set\n",
        "\n",
        "We'll start by generating all balanced sequences of 20 parentheses. We will use a 3-token vocabulary, composed of the tokens '(', ')', and '_'. '_' will be used as a padding and end-of-sentence token."
      ],
      "metadata": {
        "id": "RmG8HfJjALG9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2TDxOwPAHkv",
        "outputId": "d5f48e84-358c-405c-8030-820d6281631e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['((((((((((()))))))))))_', '(((((((((()())))))))))_', '(((((((((())()))))))))_', '(((((((((()))())))))))_', '(((((((((())))()))))))_', '(((((((((()))))())))))_', '(((((((((())))))()))))_', '(((((((((()))))))())))_', '(((((((((())))))))()))_', '(((((((((()))))))))())_']\n",
            "82499\n"
          ]
        }
      ],
      "source": [
        "def is_balanced(seq):\n",
        "  seq = seq.replace('_','')\n",
        "  s = 0\n",
        "  for x in seq:\n",
        "    s += 1 if x == '(' else -1\n",
        "    if s < 0:\n",
        "      return False\n",
        "  return s == 0\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join(['_()'[int(x)] for x in seq])\n",
        "\n",
        "def encode(seqs):\n",
        "    maxlen = max([len(s) for s in seqs])\n",
        "    out = []\n",
        "    att_mask = []\n",
        "    for s in seqs:\n",
        "        enc = [{'(': 1, ')': 2, '_': 0}[x] for x in s]\n",
        "        att_mask.append([0]*(maxlen-len(enc))+[1]*len(enc))\n",
        "        enc = [0]*(maxlen-len(enc)) + enc\n",
        "        out.append(enc)\n",
        "    return torch.tensor(out, dtype=torch.long), torch.tensor(att_mask, dtype=torch.long)\n",
        "\n",
        "def generate_positive_examples(max_length):\n",
        "    init = [1]\n",
        "    def next_token(current, results, max_length):\n",
        "        if len(current) == max_length:\n",
        "            if sum(current) == 0:\n",
        "                results.append(current+[0])\n",
        "            return\n",
        "        if sum(current) == max_length-len(current):\n",
        "            results.append(current+[-1]*(max_length-len(current)) + [0])\n",
        "            return\n",
        "        elif sum(current) == 0:\n",
        "            results.append(current + [0])\n",
        "            next_token(current+[1], results, max_length)\n",
        "        elif sum(current) > 0:\n",
        "            next_token(current+[1], results, max_length)\n",
        "            next_token(current+[-1], results, max_length)\n",
        "    results = []\n",
        "    next_token(init, results, max_length)\n",
        "    return list(map(decode,results))\n",
        "\n",
        "MAX_LENGTH=22\n",
        "positive_examples = generate_positive_examples(MAX_LENGTH)\n",
        "print(positive_examples[:10])\n",
        "print(len(positive_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "We use the GPT2 architecture as implemented in https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/model.py, with minor modifications."
      ],
      "metadata": {
        "id": "jdclJz-gNavF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        super(Conv1D, self).__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = Parameter(w)\n",
        "        self.bias = Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, config, scale=False):\n",
        "        super(Attention, self).__init__()\n",
        "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
        "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
        "        assert n_state % config.n_head == 0\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.n_head = config.n_head\n",
        "        self.split_size = n_state\n",
        "        self.scale = scale\n",
        "        self.c_attn = Conv1D(n_state * 3, nx)\n",
        "        self.c_proj = Conv1D(n_state, nx)\n",
        "\n",
        "    def _attn(self, q, k, v, attention_mask):\n",
        "        w = torch.matmul(q, k)\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "        nd, ns = w.size(-2), w.size(-1)\n",
        "        b = self.bias[:, :, ns-nd:ns, :ns]\n",
        "        w = w * b - 1e10 * (1 - b)\n",
        "        w = w + attention_mask\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "        return torch.matmul(w, v)\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
        "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
        "\n",
        "    def split_heads(self, x, k=False):\n",
        "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
        "        if k:\n",
        "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
        "        else:\n",
        "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
        "\n",
        "    def forward(self, x, attention_mask=None, layer_past=None):\n",
        "        if attention_mask==None:\n",
        "          attention_mask = torch.ones_like(x)\n",
        "\n",
        "        x = self.c_attn(x)\n",
        "        query, key, value = x.split(self.split_size, dim=2)\n",
        "        query = self.split_heads(query)\n",
        "        key = self.split_heads(key, k=True)\n",
        "        value = self.split_heads(value)\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
        "            key = torch.cat((past_key, key), dim=-1)\n",
        "            value = torch.cat((past_value, value), dim=-2)\n",
        "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
        "        a = self._attn(query, key, value, attention_mask)\n",
        "        a = self.merge_heads(a)\n",
        "        a = self.c_proj(a)\n",
        "        return a, present\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
        "        super(MLP, self).__init__()\n",
        "        nx = config.n_embd\n",
        "        self.c_fc = Conv1D(n_state, nx)\n",
        "        self.c_proj = Conv1D(nx, n_state)\n",
        "        self.act = torch.relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.act(self.c_fc(x))\n",
        "        h2 = self.c_proj(h)\n",
        "        return h2\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_ctx, config, scale=False):\n",
        "        super(Block, self).__init__()\n",
        "        nx = config.n_embd\n",
        "        #self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "        self.attn = Attention(nx, n_ctx, config, scale)\n",
        "        #self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = MLP(4 * nx, config)\n",
        "\n",
        "    def forward(self, x, attention_mask, layer_past=None):\n",
        "        if attention_mask == None:\n",
        "          attention_mask = torch.ones_like(x)\n",
        "\n",
        "        #a, present = self.attn(self.ln_1(x), layer_past=layer_past)\n",
        "        a, present = self.attn(x, attention_mask, layer_past=layer_past)\n",
        "        x = x + a\n",
        "        #m = self.mlp(self.ln_2(x))\n",
        "        m = self.mlp(x)\n",
        "        x = x + m\n",
        "        return x, present\n",
        "\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2Model, self).__init__()\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_vocab = config.vocab_size\n",
        "\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        block = Block(config.n_ctx, config, scale=True)\n",
        "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n",
        "        #self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def set_embeddings_weights(self, model_embeddings_weights):\n",
        "        embed_shape = model_embeddings_weights.shape\n",
        "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
        "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, position_ids=None, token_type_ids=None, past=None):\n",
        "        if past is None:\n",
        "            past_length = 0\n",
        "            past = [None] * len(self.h)\n",
        "        else:\n",
        "            past_length = past[0][0].size(-2)\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long,\n",
        "                                        device=input_ids.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        attention_mask = attention_mask.view(attention_mask.shape[0], -1)\n",
        "        attention_mask = attention_mask[:, None, None, :]\n",
        "        attention_mask = (1.0 - attention_mask) * -1e30\n",
        "\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        position_ids = position_ids.view(-1, position_ids.size(-1))\n",
        "\n",
        "        inputs_embeds = self.wte(input_ids)\n",
        "        position_embeds = self.wpe(position_ids)\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
        "            token_type_embeds = self.wte(token_type_ids)\n",
        "        else:\n",
        "            token_type_embeds = 0\n",
        "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
        "        presents = []\n",
        "        for block, layer_past in zip(self.h, past):\n",
        "            hidden_states, present = block(hidden_states, attention_mask, layer_past)\n",
        "            presents.append(present)\n",
        "        #hidden_states = self.ln_f(hidden_states)\n",
        "        output_shape = input_shape + (hidden_states.size(-1),)\n",
        "        return hidden_states.view(*output_shape), presents\n",
        "\n",
        "class GPT2LMHead(nn.Module):\n",
        "    def __init__(self, model_embeddings_weights, config):\n",
        "        super(GPT2LMHead, self).__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.set_embeddings_weights(model_embeddings_weights)\n",
        "\n",
        "    def set_embeddings_weights(self, model_embeddings_weights):\n",
        "        embed_shape = model_embeddings_weights.shape\n",
        "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
        "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        # Truncated Language modeling logits (we remove the last token)\n",
        "        # h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n",
        "        lm_logits = self.decoder(hidden_state)\n",
        "        return lm_logits\n",
        "\n",
        "class GPT2LMHeadModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2LMHeadModel, self).__init__()\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n",
        "\n",
        "    def set_tied(self):\n",
        "        \"\"\" Make sure we are sharing the embeddings\n",
        "        \"\"\"\n",
        "        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, lm_labels=None, position_ids=None, token_type_ids=None, past=None):\n",
        "        hidden_states, presents = self.transformer(input_ids, attention_mask, position_ids, token_type_ids, past)\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        if lm_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(lm_logits[:,:-1,:].reshape(-1, lm_logits.size(-1)), lm_labels[:,1:].reshape(-1))\n",
        "            return loss\n",
        "        return lm_logits\n"
      ],
      "metadata": {
        "id": "02ldkeZwXek2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model has 3 layers, 4 attention heads and width 32"
      ],
      "metadata": {
        "id": "31vim19NImlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Config = namedtuple('Config', [\n",
        "    'vocab_size',\n",
        "    'n_positions',\n",
        "    'n_ctx',\n",
        "    'n_embd',\n",
        "    'n_layer',\n",
        "    'n_head',\n",
        "])\n",
        "\n",
        "model = GPT2LMHeadModel(Config(vocab_size=3, n_positions=32, n_ctx=1024, n_embd=32, n_layer=3, n_head=4))"
      ],
      "metadata": {
        "id": "hl3mCFt1SuWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "First we test the model on the test set (1000 sequences selected randomly) to see the untrained prediction performance."
      ],
      "metadata": {
        "id": "_UFrNXVA6i8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint, seed \n",
        "\n",
        "seed(0)\n",
        "n_test = 1000\n",
        "test_set = [positive_examples.pop(randint(0,len(positive_examples))) for i in range(n_test)]\n",
        "inp, att_mask = encode(test_set)\n",
        "loss = float(model(inp, att_mask, lm_labels=inp).detach())\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUEnFQYJbnin",
        "outputId": "009f9756-f76f-45d8-83b1-acff48d6d4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.50577163696289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we train it on the training set (the rest of the sequences) for a single epoch."
      ],
      "metadata": {
        "id": "-rWnHUn56tgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.AdamW(model.parameters())\n",
        "\n",
        "data  = len(positive_examples)\n",
        "bs = 32\n",
        "epochs = 1\n",
        "for e in range(epochs):\n",
        "  print('Epoch ',e)\n",
        "  for b in range(data//bs):\n",
        "      if b % 10 == 0:\n",
        "          print(b*bs)\n",
        "      inp, att_mask = encode(positive_examples[b*bs:(b+1)*bs])\n",
        "      loss = model(inp, att_mask, lm_labels=inp)\n",
        "      optim.zero_grad()\n",
        "      loss.backward()\n",
        "      optim.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve3GwP3kS3Ss",
        "outputId": "e8e5e0ed-16ab-4156-cf5e-a1457b38b744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0\n",
            "0\n",
            "320\n",
            "640\n",
            "960\n",
            "1280\n",
            "1600\n",
            "1920\n",
            "2240\n",
            "2560\n",
            "2880\n",
            "3200\n",
            "3520\n",
            "3840\n",
            "4160\n",
            "4480\n",
            "4800\n",
            "5120\n",
            "5440\n",
            "5760\n",
            "6080\n",
            "6400\n",
            "6720\n",
            "7040\n",
            "7360\n",
            "7680\n",
            "8000\n",
            "8320\n",
            "8640\n",
            "8960\n",
            "9280\n",
            "9600\n",
            "9920\n",
            "10240\n",
            "10560\n",
            "10880\n",
            "11200\n",
            "11520\n",
            "11840\n",
            "12160\n",
            "12480\n",
            "12800\n",
            "13120\n",
            "13440\n",
            "13760\n",
            "14080\n",
            "14400\n",
            "14720\n",
            "15040\n",
            "15360\n",
            "15680\n",
            "16000\n",
            "16320\n",
            "16640\n",
            "16960\n",
            "17280\n",
            "17600\n",
            "17920\n",
            "18240\n",
            "18560\n",
            "18880\n",
            "19200\n",
            "19520\n",
            "19840\n",
            "20160\n",
            "20480\n",
            "20800\n",
            "21120\n",
            "21440\n",
            "21760\n",
            "22080\n",
            "22400\n",
            "22720\n",
            "23040\n",
            "23360\n",
            "23680\n",
            "24000\n",
            "24320\n",
            "24640\n",
            "24960\n",
            "25280\n",
            "25600\n",
            "25920\n",
            "26240\n",
            "26560\n",
            "26880\n",
            "27200\n",
            "27520\n",
            "27840\n",
            "28160\n",
            "28480\n",
            "28800\n",
            "29120\n",
            "29440\n",
            "29760\n",
            "30080\n",
            "30400\n",
            "30720\n",
            "31040\n",
            "31360\n",
            "31680\n",
            "32000\n",
            "32320\n",
            "32640\n",
            "32960\n",
            "33280\n",
            "33600\n",
            "33920\n",
            "34240\n",
            "34560\n",
            "34880\n",
            "35200\n",
            "35520\n",
            "35840\n",
            "36160\n",
            "36480\n",
            "36800\n",
            "37120\n",
            "37440\n",
            "37760\n",
            "38080\n",
            "38400\n",
            "38720\n",
            "39040\n",
            "39360\n",
            "39680\n",
            "40000\n",
            "40320\n",
            "40640\n",
            "40960\n",
            "41280\n",
            "41600\n",
            "41920\n",
            "42240\n",
            "42560\n",
            "42880\n",
            "43200\n",
            "43520\n",
            "43840\n",
            "44160\n",
            "44480\n",
            "44800\n",
            "45120\n",
            "45440\n",
            "45760\n",
            "46080\n",
            "46400\n",
            "46720\n",
            "47040\n",
            "47360\n",
            "47680\n",
            "48000\n",
            "48320\n",
            "48640\n",
            "48960\n",
            "49280\n",
            "49600\n",
            "49920\n",
            "50240\n",
            "50560\n",
            "50880\n",
            "51200\n",
            "51520\n",
            "51840\n",
            "52160\n",
            "52480\n",
            "52800\n",
            "53120\n",
            "53440\n",
            "53760\n",
            "54080\n",
            "54400\n",
            "54720\n",
            "55040\n",
            "55360\n",
            "55680\n",
            "56000\n",
            "56320\n",
            "56640\n",
            "56960\n",
            "57280\n",
            "57600\n",
            "57920\n",
            "58240\n",
            "58560\n",
            "58880\n",
            "59200\n",
            "59520\n",
            "59840\n",
            "60160\n",
            "60480\n",
            "60800\n",
            "61120\n",
            "61440\n",
            "61760\n",
            "62080\n",
            "62400\n",
            "62720\n",
            "63040\n",
            "63360\n",
            "63680\n",
            "64000\n",
            "64320\n",
            "64640\n",
            "64960\n",
            "65280\n",
            "65600\n",
            "65920\n",
            "66240\n",
            "66560\n",
            "66880\n",
            "67200\n",
            "67520\n",
            "67840\n",
            "68160\n",
            "68480\n",
            "68800\n",
            "69120\n",
            "69440\n",
            "69760\n",
            "70080\n",
            "70400\n",
            "70720\n",
            "71040\n",
            "71360\n",
            "71680\n",
            "72000\n",
            "72320\n",
            "72640\n",
            "72960\n",
            "73280\n",
            "73600\n",
            "73920\n",
            "74240\n",
            "74560\n",
            "74880\n",
            "75200\n",
            "75520\n",
            "75840\n",
            "76160\n",
            "76480\n",
            "76800\n",
            "77120\n",
            "77440\n",
            "77760\n",
            "78080\n",
            "78400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we test the prediction loss again to check that the model has learned something."
      ],
      "metadata": {
        "id": "LIaM05OK652g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp_test, att_mask = encode(test_set)\n",
        "loss_test = float(model(inp_test, att_mask, lm_labels=inp_test).detach())\n",
        "print(\"Loss in test dataset: \", loss_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-y6-OKmce4-",
        "outputId": "0376b011-c628-4678-b984-c8269deab94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss in test dataset:  1.6770515441894531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "We prompt the model with the initial portion of a balanced sequence and check how often it outputs a balanced completion. This should take less than a minute."
      ],
      "metadata": {
        "id": "i9H21LIC7B6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = []\n",
        "for lim in range(1,13):\n",
        "    print(f'{lim}/13')\n",
        "    prompts = list({x[:lim] for x in positive_examples})\n",
        "    inp, att_mask = encode(prompts)\n",
        "    results = []\n",
        "    for idx in range(inp.shape[0]):\n",
        "        lim = 10\n",
        "        ex = inp[idx].reshape(1,-1)\n",
        "        atm = att_mask[idx].reshape(1,-1)\n",
        "        for i in range(MAX_LENGTH):\n",
        "            logits = model(ex, atm)[0]\n",
        "            next_tok = logits.argmax(1)[-1].detach().reshape(1,1)\n",
        "            ex = torch.concat([ex, next_tok],dim=1)\n",
        "            atm = torch.concat([atm, torch.ones(1,1)], dim=1)\n",
        "            if int(next_tok[0,0].cpu()) == 0:\n",
        "                break\n",
        "        out = decode(ex[0])\n",
        "        results.append(is_balanced(out))\n",
        "    res.append(sum(results)/len(results))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(list(range(1,13)), res)\n",
        "plt.xlabel('Length of the prompt')\n",
        "plt.ylabel('Percentage of successful completions')\n",
        "plt.ylim(0,1.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ZrtajlemciV5",
        "outputId": "4d18b9a2-e158-42cd-afdb-c7130a9ce49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is not perfect, but it achieves a reasonable accuracy. With a larger model, and more iterations, it's plausible that we could reach 100% accuracy on our dataset.\n",
        "\n",
        "However, we would still not know if the model also behaves as expected on longer sequences outside our dataset. To ensure this we need verification.\n",
        "\n",
        "# Verification\n",
        "\n",
        "To verify our model, we'll define a perturbation set on the input space and find bounds for the output within this set. The set is defined by the following linear bounds, where x_i is the encoding of the ith token.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "We define a the optimization problem to be solved to find the bound within this set and wrap it in a Perturbation class for use with the auto_LiRPA library."
      ],
      "metadata": {
        "id": "tVXmax6cJwKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from auto_LiRPA.operators.softmax import PerturbationL0Norm\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "class PerturbationParenthesis():\n",
        "    def __init__(self, x, eps=1):\n",
        "        self.eps = set_eps\n",
        "        self.x = x\n",
        "\n",
        "    def concretize_helper(self, x, A, sign=-1, aux=None):\n",
        "        if type(A) != np.ndarray:\n",
        "            A = A.to_numpy()\n",
        "        n = x.shape[1]\n",
        "        constrain_mat = (-2)*np.ones(n*n).reshape((n,n))\n",
        "        constrain_mat = np.tril(constrain_mat)\n",
        "        constrain_b = 3*(np.arange(n) + 1)\n",
        "        x_i_bound = (0,2)\n",
        "        bound = linprog(sign*A, -constrain_mat[1:,:], constrain_b[1:], bounds=n*[x_i_bound])\n",
        "        return bound.fun\n",
        "\n",
        "\n",
        "    def concretize(self, x, A, sign=-1, aux=None):\n",
        "        bound = []\n",
        "        for i in range(A.shape[0]):\n",
        "            bound.append(concretize_helper(x, A[i, :], sign=sign))\n",
        "        return torch.FloatTensor(bound)\n",
        "\n",
        "    # I took this from the L0-norm perturbation\n",
        "    def init(self, x, aux=None, forward=False):\n",
        "        # For other norms, we pass in the BoundedTensor objects directly.\n",
        "        x_L = x\n",
        "        x_U = x\n",
        "        if not forward:\n",
        "            return LinearBound(None, None, None, None, x_L, x_U), x, None\n",
        "        else:\n",
        "            raise NotImplementedError()\n"
      ],
      "metadata": {
        "id": "UdO0efQ_L66_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "98cc5c65-4835-4570-98b5-7ee75ac54ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-70a5e11ffc17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinprog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPerturbationParenthesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPerturbation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Perturbation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, at this point the verification library fails. This is because some operations in our model are not well supported, which is understandable since even our simplified version of GPT2 is still quite complex."
      ],
      "metadata": {
        "id": "66fqIBCQOCiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the model with auto_LiRPA.\n",
        "Bmodel = BoundedModule(model, (torch.zeros(1,32).long(), torch.zeros(1,32).long()))\n",
        "\n",
        "# Define perturbation. Here we add Linf perturbation to input data.\n",
        "ptb = PerturbationLpNorm(norm=np.inf, eps=1.1)\n",
        "\n",
        "# Make the input a BoundedTensor with the pre-defined perturbation.\n",
        "input = BoundedTensor(inp_test[0], ptb)\n",
        "atm = att_mask[0]\n",
        "\n",
        "# Compute LiRPA bounds using the backward mode bound propagation (CROWN).\n",
        "lb, ub = Bmodel.compute_bounds(x=(input, atm), method=\"backward\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Md1Gc5yhJWxo",
        "outputId": "c8230e47-84fe-4162-90a1-d91bdcda7f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ca8523089509>:53: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
            "<ipython-input-3-ca8523089509>:39: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  w = w / math.sqrt(v.size(-1))\n",
            "/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py:970: UserWarning: Creating an identity matrix with size 1024x1024 for node BoundTranspose(name=\"/key.3\"). This may indicate poor performance for bound computation. If you see this message on a small network please submit a bug report.\n",
            "  sparse_C = self.get_sparse_C(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2a2bde3de3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Compute LiRPA bounds using the backward mode bound propagation (CROWN).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"backward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcompute_bounds\u001b[0;34m(self, x, aux, C, method, IBP, forward, bound_lower, bound_upper, reuse_ibp, reuse_alpha, return_A, needed_A_dict, final_node_name, average_A, intermediate_layer_bounds, reference_bounds, intermediate_constr, alpha_idx, aux_reference_bounds, need_A_only, cutter, decision_thresh, update_mask)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_node_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'backward'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             self.compute_intermediate_bounds(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcheck_prior_bounds\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prior_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'requires_input_bounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             self.compute_intermediate_bounds(\n\u001b[0m\u001b[1;32m    886\u001b[0m                 node.inputs[i], prior_checked=True)\n\u001b[1;32m    887\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_checked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/bound_general.py\u001b[0m in \u001b[0;36mcompute_intermediate_bounds\u001b[0;34m(self, node, prior_checked)\u001b[0m\n\u001b[1;32m    981\u001b[0m                         \u001b[0;31m# Compute backward bounds only when there are unstable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                         \u001b[0;31m# neurons, or when we don't know which neurons are unstable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m                         node.lower, node.upper = self.backward_general(\n\u001b[0m\u001b[1;32m    984\u001b[0m                             \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munstable_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munstable_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m                             unstable_size=unstable_size)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/backward_bound.py\u001b[0m in \u001b[0;36mbackward_general\u001b[0;34m(self, C, node, bound_lower, bound_upper, average_A, need_A_only, unstable_idx, unstable_size, update_mask, verbose)\u001b[0m\n\u001b[1;32m    157\u001b[0m                     l.lA, l.uA, *l.inputs, start_shape=start_shape, start_node=node)\n\u001b[1;32m    158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbound_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;31m# After propagation through this node, we delete its lA, uA variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_A\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/operators/shape.py\u001b[0m in \u001b[0;36mbound_backward\u001b[0;34m(self, last_lA, last_uA, x, shape)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbound_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mlw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0muw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/auto_LiRPA/operators/shape.py\u001b[0m in \u001b[0;36m_bound_oneside\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_bound_oneside\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_lA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_bound_oneside\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_uA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mbound_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mlw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1024, 23, 96]' is invalid for input of size 72351744"
          ]
        }
      ]
    }
  ]
}